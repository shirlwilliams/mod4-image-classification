{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Image Classification with Deep Learning - Pneumonia\n",
    "Module 4 Project<br>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Objective and Goal: \n",
    "\n",
    "<b>Objective</b>: Given a chest x-ray image of pediatric patients, build a model that can classify whether a given patient has pneumonia.<br><br>\n",
    "<b>Goal</b>: Build a deep neural network trained on a large dataset for classification on a non-trivial task.<br><br>\n",
    "Use x-ray images of pediatric patients to identify whether or not they have pneumonia. The datasets come from <a href=\"https://www.kaggle.com/paultimothymooney/chest-xray-pneumonia\">Kaggle</a>. With Deep Learning, data is king -- the more of it, the better. However, the goal of this project isn't to build the best model possible -- it's to demonstrate my understanding by building a model that works. <br>\n",
    "\n",
    "To aid in processing time, I used Google Colab to create my models. \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Outline<img src=\"images/child-pneumonia.png\" alt=\"sick child\" ALIGN=\"right\"/>\n",
    "* The Data\n",
    "* The Model\n",
    "    * Model 1-2 Neural Networks\n",
    "    * Model 3 First CNN\n",
    "    * Model 4 CNN with varied values\n",
    "    * Model 5 CNN with Dropouts added\n",
    "    * Model 6 CNN with L1 Regularizer\n",
    "    * Model 7 CNN with L2 Regularizer\n",
    "* Conclusion\n",
    "    * Other Considerations\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": true
   },
   "source": [
    "# The Data\n",
    "The number of images for the train set were extremely unbalanced: \n",
    "* Normal images - 84\n",
    "* Pneumonia images - 1134<br><br>\n",
    "The number of images for the test set were more balanced: \n",
    "* Normal images - 234\n",
    "* Pneumonia images - 390<br><br>\n",
    "The pneumonia images were split evenly between viral pneumonia and bacterial pneumonia.\n",
    "\n",
    "<br><br>\n",
    "\n",
    "\n",
    "<table>\n",
    "    <tr>\n",
    "        <td><img src='images/NORMAL2-IM-1258-0001.jpeg' alt=\"normal chest x-ray\" style=\"width: 300px;\"></td>\n",
    "        <td><img src='images/person1568_virus_2723.jpeg' alt=\"viral pneumonia chest x-ray\" style=\"width: 350px;\"></td>\n",
    "        <td><img src=\"images/person1945_bacteria_4872.jpeg\" alt=\"bacterial pneumonia chest x-ray\" style=\"width: 350px;\"/></td>\n",
    "    </tr>\n",
    "    <tr>\n",
    "        <td align=\"left\"><figcaption>Image of a Normal Chest X-ray</figcaption></td>\n",
    "        <td align=\"left\"><figcaption>Image of a Viral Pneumonia Chest X-ray</figcaption></td>\n",
    "        <td align=\"left\"><figcaption>Image of a Bacterial Pneumonia Chest X-ray</figcaption></td>\n",
    "    </tr>\n",
    "</table>\n",
    "\n",
    "\n",
    "Pneumonia in children can be difficult to diagnose with x-ray images as illustrated by examining the x-rays above. Physicians may have difficulty determining differences particularly when many images must be evaluated. Convolutional neural networks have proven to aid in a diagnoses.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": true
   },
   "source": [
    "# The Models\n",
    "Initially normal neural networks were used but determined to be insufficient. Neural networks are trained using the backpropagation of an error algorithm that involves calculating errors made by the model on the training dataset and updating the model weights in proportion to those errors. The limitation of this method of training is that examples from each class are treated the same, which for imbalanced datasets means that the model has adapted a lot more for one class than another.\n",
    "\n",
    "Because of the imbalance on the train set, ultimately a <b>Convolutional Neural Network</b> where augmentation was used on the train set using arguments of the ImageGenerator to bring up the number of train images. A convolution neural network is similar to a multi-layer perceptron network. The major differences are what the network learns, how they are structured and what purpose they are mostly used. CNNs are largely applied in the domain of computer vision and has been highly successful in achieving state of the art performance on various test cases.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true
   },
   "source": [
    "Using the ImageDataGenerator the following arguments were adjusted to aid in image augmentation.<br>\n",
    "```python\n",
    "train_generator = ImageDataGenerator(    \n",
    "    shear_range=0.2,\n",
    "    zoom_range=0.2,\n",
    "    horizontal_flip=True,\n",
    "    ).flow_from_directory(\n",
    "        train_data_dir, \n",
    "        target_size=(64, 64), \n",
    "        batch_size=1218,\n",
    "        class_mode='binary',\n",
    "        shuffle=True,\n",
    "        seed=42)\n",
    "```\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true
   },
   "source": [
    "## Models 1-2 tried differing nueral networks\n",
    "Before learning Convolutional Neural Networks are the best practices for image classification I tried normal neural networks without much success. Note: The second model used class weights but didn't budge in imbalanced results.\n",
    "\n",
    "### Results:\n",
    "* Training Accuracy: 0.93 \n",
    "* Training Error: 0.07\n",
    "* Test Accuracy: 0.62 \n",
    "* Test Error: 0.38 <br>\n",
    "<img src=\"images/CM Model1.png\" alt=\"confusion matrix\" ALIGN=\"left\"/>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true
   },
   "source": [
    "## Model 3\n",
    "In the third model a basic CNN was tried using Conv2D, MaxPooling2D, and Flatten along with a weighted class. Note: I tried using other optimizers but adam proved better.\n",
    "```python\n",
    "# Third model using a CNN\n",
    "classifier = Sequential([\n",
    "                         Conv2D(8, kernel_size=(5, 5), activation='relu', padding='same',input_shape=train_images.shape[1:]),\n",
    "                         MaxPooling2D(pool_size=(2, 2),strides=2),\n",
    "                         Conv2D(4, kernel_size=(5, 5), activation='relu'),\n",
    "                         MaxPooling2D(pool_size=(2, 2),strides=2),\n",
    "                         Flatten(),\n",
    "                         Dense(16, activation='relu'),\n",
    "                         Dense(8, activation='relu'),\n",
    "                         Dense(1, activation='sigmoid')\n",
    "                         ])\n",
    "# Compile the model\n",
    "classifier.compile(optimizer='adam', \n",
    "                   loss='binary_crossentropy', \n",
    "                   metrics=['accuracy'])\n",
    "\n",
    "# Train the model\n",
    "model3 = classifier.fit(train_images, \n",
    "                        train_labels, \n",
    "                        class_weight=weights,\n",
    "                        epochs=15, \n",
    "                        batch_size=10)\n",
    "```\n",
    "\n",
    "### Results of the first CNN model \n",
    "Disappointing first cnn model results that are identical to models 1 and 2.\n",
    "* Training Accuracy: 0.93 \n",
    "* Training Error: 0.07\n",
    "* Test Accuracy: 0.62 \n",
    "* Test Error: 0.38 <br>\n",
    "<img src=\"images/CM Model1.png\" alt=\"confusion matrix\" ALIGN=\"left\"/>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true
   },
   "source": [
    "## Model 4\n",
    "A new model with different numbers. Literally 2 different numbers made a big difference.\n",
    "```python\n",
    "# Use of CNN with different kernel values\n",
    "classifier = Sequential([\n",
    "                         Conv2D(8, kernel_size=(4, 4), activation='relu', padding='same',input_shape=train_images.shape[1:]),\n",
    "                         MaxPooling2D(pool_size=(2, 2),strides=2),\n",
    "                         Conv2D(4, kernel_size=(5, 5), activation='relu'),\n",
    "                         MaxPooling2D(pool_size=(2, 2),strides=2),\n",
    "                         Flatten(),\n",
    "                         Dense(16, activation='relu'),\n",
    "                         Dense(8, activation='relu'),\n",
    "                         Dense(1, activation='sigmoid')\n",
    "                         ])\n",
    "# Compile the model\n",
    "classifier.compile(optimizer='adam', \n",
    "                   loss='binary_crossentropy', \n",
    "                   metrics=['accuracy'])\n",
    "\n",
    "# Train the model\n",
    "model4 = classifier.fit(train_images, \n",
    "                        train_labels, \n",
    "                        class_weight=weights,\n",
    "                        epochs=15, \n",
    "                        batch_size=8)\n",
    "```\n",
    "### Results of model 4\n",
    "\n",
    "* Training: \n",
    "    * Accuracy 0.96\n",
    "    * Error: 0.04\n",
    "* Test:\n",
    "    * Accuracy: 0.74 \n",
    "    * Error: 0.26\n",
    "    * Precision: 0.71 \n",
    "    * Recall: 0.98 \n",
    "    * F1 score: 0.83<br>\n",
    "<img src=\"images/CM Model4.png\" alt=\"confusion matrix\" ALIGN=\"left\"/>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true
   },
   "source": [
    "## Model 5\n",
    "```python\n",
    "# Use of CNN with an L1 optimizer\n",
    "classifier = Sequential([\n",
    "                         Conv2D(8, kernel_size=(4, 4), activation='relu', padding='same',input_shape=train_images.shape[1:]),\n",
    "                         MaxPooling2D(pool_size=(2, 2),strides=2),\n",
    "                         Conv2D(4, kernel_size=(5, 5), activation='relu'),\n",
    "                         MaxPooling2D(pool_size=(2, 2),strides=2),\n",
    "                         Dropout(0.3),\n",
    "                         Flatten(),\n",
    "                         Dense(16, activation='relu'),\n",
    "                         Dropout(0.3),\n",
    "                         Dense(8, activation='relu'),\n",
    "                         Dense(1, activation='sigmoid')\n",
    "                         ])\n",
    "# Compile the model\n",
    "classifier.compile(optimizer='adam', \n",
    "                   loss='binary_crossentropy', \n",
    "                   metrics=['accuracy'])\n",
    "\n",
    "# Train the model\n",
    "model5 = classifier.fit(train_images,\n",
    "                        train_labels, \n",
    "                        class_weight=weights,\n",
    "                        epochs=50,\n",
    "                        batch_size=10)\n",
    "```\n",
    "### Results of model 5\n",
    "\n",
    "* Training: \n",
    "    * Accuracy 0.94\n",
    "    * Error: 0.06\n",
    "* Test:\n",
    "    * Accuracy: 0.79 \n",
    "    * Error: 0.21\n",
    "    * Precision: 0.77 \n",
    "    * Recall: 0.95 \n",
    "    * F1 score: 0.85<br>\n",
    "\n",
    "<img src=\"images/CM Model5.png\" alt=\"confusion matrix\" ALIGN=\"left\"/>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": true,
    "hidden": true
   },
   "source": [
    "## Model 6\n",
    "This CNN model used an L1 regularizer.\n",
    "```python\n",
    "# Use of CNN with L1 added\n",
    "classifier = Sequential([\n",
    "                         Conv2D(8, kernel_size=(4, 4), activation='relu', \n",
    "                                padding='same',input_shape=train_images.shape[1:],\n",
    "                                kernel_regularizer=regularizers.l1(0.005)),\n",
    "                         MaxPooling2D(pool_size=(2, 2),strides=2),\n",
    "                         Conv2D(4, kernel_size=(5, 5), activation='relu', kernel_regularizer=regularizers.l1(0.005)),\n",
    "                         MaxPooling2D(pool_size=(2, 2),strides=2),\n",
    "                         Dropout(0.3),\n",
    "                         Flatten(),\n",
    "                         Dense(16, activation='relu', kernel_regularizer=regularizers.l1(0.005)),\n",
    "                         Dropout(0.3),\n",
    "                         Dense(8, activation='relu'),\n",
    "                         Dense(1, activation='sigmoid')\n",
    "                         ])\n",
    "# Compile the model\n",
    "classifier.compile(optimizer='adam', \n",
    "                   loss='binary_crossentropy', \n",
    "                   metrics=['accuracy'])\n",
    "\n",
    "# Train the model\n",
    "model6 = classifier.fit(train_images,\n",
    "                        train_labels, \n",
    "                        class_weight=weights,\n",
    "                        epochs=50,\n",
    "                        batch_size=10)\n",
    "```\n",
    "### Results of model 6\n",
    "\n",
    "* Training: \n",
    "    * Accuracy 0.89\n",
    "    * Error: 0.11\n",
    "* Test:\n",
    "    * Accuracy: 0.71 \n",
    "    * Error: 0.29\n",
    "    * Precision: 0.70 \n",
    "    * Recall: 0.95 \n",
    "    * F1 score: 0.80<br>\n",
    "\n",
    "<img src=\"images/CM Model6.png\" alt=\"confusion matrix\" ALIGN=\"left\"/>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true
   },
   "source": [
    "## Model 7\n",
    "This cnn model uses an L1 regularizer to assist with overfitting.\n",
    "```python\n",
    "# Use of CNN with L2 added\n",
    "classifier = Sequential([\n",
    "                         Conv2D(8, kernel_size=(4, 4), activation='relu', \n",
    "                                padding='same',input_shape=train_images.shape[1:],\n",
    "                                kernel_regularizer=regularizers.l2(0.01)),\n",
    "                         MaxPooling2D(pool_size=(2, 2),strides=2),\n",
    "                         Conv2D(4, kernel_size=(5, 5), activation='relu', kernel_regularizer=regularizers.l2(0.01)),\n",
    "                         MaxPooling2D(pool_size=(2, 2),strides=2),\n",
    "                         Dropout(0.3),\n",
    "                         Flatten(),\n",
    "                         Dense(16, activation='relu', kernel_regularizer=regularizers.l2(0.01)),\n",
    "                         Dropout(0.3),\n",
    "                         Dense(8, activation='relu'),\n",
    "                         Dense(1, activation='sigmoid')\n",
    "                         ])\n",
    "# Compile the model\n",
    "classifier.compile(optimizer='adam', \n",
    "                   loss='binary_crossentropy', \n",
    "                   metrics=['accuracy'])\n",
    "\n",
    "# Train the model\n",
    "model7 = classifier.fit(train_images,\n",
    "                        train_labels, \n",
    "                        class_weight=weights,\n",
    "                        epochs=50,\n",
    "                        batch_size=10)\n",
    "```\n",
    "### Results of model 7\n",
    "\n",
    "* Training: \n",
    "    * Accuracy 0.95\n",
    "    * Error: 0.05\n",
    "* Test:\n",
    "    * Accuracy: 0.77 \n",
    "    * Error: 0.23\n",
    "    * Precision: 0.77 \n",
    "    * Recall: 0.91 \n",
    "    * F1 score: 0.83<br>\n",
    "\n",
    "<img src=\"images/CM Model7.png\" alt=\"confusion matrix\" ALIGN=\"left\"/>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Conclusions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Of the models found Model 5 tends to have the higher group of scores; higher accuracy, precision, f1 and more importantly the higher recall. \n",
    "\n",
    "* Convolution neural networks are most useful classifying images over regular neural networks\n",
    "* Dropout is a preferable regularization technique to avoid overfitting in deep neural networks.\n",
    "* L1 (Lasso Regression) regularization adds a “squared magnitude” of the coefficient as a penalty term to the loss function. \n",
    "* L2 (Ridge Regression) regularization adds an \"absolute value of magnitude\" of the coefficient as a penalty term to the loss function.\n",
    "* Google colab aids time on task - so much faster\n",
    "\n",
    "## Future options\n",
    "* Add cross-validation and possibly a grid search to tune arguments.\n",
    "* Find a dataset with more balanced data or add to the normal x-ray training dataset.\n",
    "* Continue to tune parameters/hyperparameters.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:learn-env] *",
   "language": "python",
   "name": "conda-env-learn-env-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": false,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
